import time
import gspread
import os
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ì „ì—­ ë³€ìˆ˜
url_mapping = {}

def parse_period(period_text):
    # 1) ì¤„ë°”ê¿ˆ ì œê±°
    clean = period_text.replace("\n", "").replace("\r", "")
    # 2) ê´„í˜¸ ì•ˆ ì„¤ëª… ëª¨ë‘ ì œê±°
    clean = re.sub(r"\([^)]*\)", "", clean)
    # 3) ê³µë°±(ìŠ¤í˜ì´ìŠ¤) ì „ë¶€ ì œê±°
    clean = clean.replace(" ", "")
    # 4) '~' ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    parts = clean.split("~")
    if len(parts) != 2:
        return "", ""
    # 5) ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜
    def to_iso(s):
        if "." not in s:
            return ""
        m, d = s.split(".")
        return f"2025-{m.zfill(2)}-{d.zfill(2)}"
    return to_iso(parts[0]), to_iso(parts[1])

# --- WebDriver ì„¤ì •
def setup_driver():
    options = Options()
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)
    return driver

# --- ê°€ê²© í…ìŠ¤íŠ¸ ì²˜ë¦¬
def process_price_text(price_text):
    if "ì •ìƒê°€" in price_text and "íŒë§¤ê°€" in price_text:
        try:
            parts = price_text.split("íŒë§¤ê°€")
            original_price = parts[0].strip()
            sale_price = parts[1].strip()
            return f"<s>{original_price}</s> íŒë§¤ê°€ {sale_price}"
        except:
            return price_text
    else:
        return price_text

# --- í–‰ì‚¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_list(driver, branchCd, page):
    list_url = f"https://www.ehyundai.com/newPortal/SN/SN_0101000.do?branchCd={branchCd}&SN=1"
    driver.get(list_url)
    time.sleep(2)
    driver.execute_script(f"getContents('01', {page}, 0);")
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("#eventList > li")

# --- í–‰ì‚¬ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_detail(driver, url):
    try:
        driver.get(url)
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # â‘  ì œëª© ê°€ì ¸ì˜¤ê¸° (fixAreas â†’ fixArea)
        title_el = soup.select_one("section.fixArea h2")
        title_text = title_el.text.strip() if title_el else ""

        # â‘¡ ê¸°ê°„ ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
        period_el = soup.select_one("table.info td")
        period_text = period_el.text.strip() if period_el else ""

        # â‘¢ ISO í¬ë§· ë‚ ì§œ íŒŒì‹± & í…ìŠ¤íŠ¸ ì„¤ëª… ìˆ˜ì§‘
        start_iso, end_iso = parse_period(period_text)

        noimg_list = [
            f"{r.find('th').text.strip()}: {r.find('td').text.strip()}"
            for r in soup.select("article.noImgProduct tr")
            if r.find('th') and r.find('td')
        ]

        products = []
        for p in soup.select("article.twoProduct figure"):
            # 1) ì¤„ë°”ê¿ˆ ì œê±°: ê³µë°± í•˜ë‚˜ë¡œ í†µì¼
            brand_text = (
                p.select_one(".p_brandNm").get_text(" ", strip=True)
                if p.select_one(".p_brandNm")
                else ""
            )
            name_text = (
                p.select_one(".p_productNm").get_text(" ", strip=True)
                if p.select_one(".p_productNm")
                else ""
            )

            # 2) í•´ì‹œíƒœê·¸ ì œê±°
            brand_text = " ".join(w for w in brand_text.split() if not w.startswith("#"))

            # 3) ë¶„ë¥˜ í† í° ì œê±°
            if brand_text.upper() in ("MEN", "WOMEN", "MEN/WOMEN"):
                brand_text = ""

            # 4) name_textê°€ ë¹„ì–´ìˆìœ¼ë©´ brand_text â†’ name_text
            if not name_text and brand_text:
                name_text, brand_text = brand_text, ""

            # 5) '[ë¸Œëœë“œ]' ë‘˜ëŸ¬ì‹¼ ê²½ìš° ëŒ€ê´„í˜¸ ì œê±°
            if re.fullmatch(r"\[[^\]]+\]", brand_text):
                brand_text = brand_text[1:-1].strip()

            # 6) brand_text ì—†ì„ ë•Œ '[ë¸Œëœë“œ]ì œí’ˆëª…' ë¶„ë¦¬
            if not brand_text:
                m = re.match(r"^\[([^\]]+)\]\s*(.+)$", name_text)
                if m:
                    brand_text = m.group(1).strip()
                    name_text = m.group(2).strip()

            # 7) ë³€í˜• ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°(ìŠ¬ë˜ì‹œ í¬í•¨) í•©ì¹˜ê¸°
            if brand_text and "/" in name_text:
                name_text = f"{brand_text} {name_text}"
                brand_text = ""

            # 8) í”„ë¡œëª¨ì…˜/ì¦ì • í•­ëª© ê±´ë„ˆë›°ê¸°
            if "ì¦ì •" in brand_text or "êµ¬ë§¤ì‹œ" in name_text or name_text.startswith("ã€Œ"):
                continue

            # 9) SKU ì½”ë“œë§Œ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if re.fullmatch(r"[A-Z0-9]+", name_text):
                continue

            # ê°€ê²© ë° ì´ë¯¸ì§€ URL
            price_tag = p.select_one(".p_productPrc")
            price_txt = price_tag.get_text(" ", strip=True) if price_tag else ""
            img_tag = p.select_one(".p_productImg")
            img_url = img_tag["src"] if img_tag else ""

            # ìµœì¢… ê³µë°± ì •ë¦¬
            name_text = " ".join(name_text.split())

            products.append({
                "ë¸Œëœë“œ": brand_text,
                "ì œí’ˆëª…": name_text,
                "ê°€ê²©": process_price_text(price_txt),
                "ì´ë¯¸ì§€": img_url
            })

        return {
            "ìƒì„¸ ì œëª©": title_text,
            "ìƒì„¸ ê¸°ê°„": period_text,
            "ì‹œì‘ì¼": start_iso,
            "ì¢…ë£Œì¼": end_iso,
            "í…ìŠ¤íŠ¸ ì„¤ëª…": noimg_list,
            "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": products
        }

    except Exception as e:
        print(f"âŒ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {"ìƒì„¸ ì œëª©": "", "ìƒì„¸ ê¸°ê°„": "", "ì‹œì‘ì¼":"", "ì¢…ë£Œì¼":"", "í…ìŠ¤íŠ¸ ì„¤ëª…": [], "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": []}
        
# --- HTML í˜ì´ì§€ ìƒì„±
def generate_html(detail_data, event_id):
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    template_path = os.path.join(BASE_DIR, "template.html")
    with open(template_path, "r", encoding="utf-8") as f:
        template = f.read()

    # í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
    html = template
    html = html.replace("{{ì œëª©}}", detail_data["ì œëª©"])
    html = html.replace("{{ê¸°ê°„}}", detail_data["ê¸°ê°„"])
    html = html.replace("{{ìƒì„¸ ì œëª©}}", detail_data["ìƒì„¸ ì œëª©"])
    html = html.replace("{{ìƒì„¸ ê¸°ê°„}}", detail_data["ìƒì„¸ ê¸°ê°„"])
    html = html.replace("{{ì¸ë„¤ì¼}}", detail_data.get("ì¸ë„¤ì¼", ""))
    html = html.replace("{{í˜œíƒ ì„¤ëª…}}", detail_data["í˜œíƒ ì„¤ëª…"].replace("\n", "<br>"))
    html = html.replace("{{ì—…ë°ì´íŠ¸ ë‚ ì§œ}}", datetime.today().strftime('%Y-%m-%d'))
    html = html.replace("{{ì‹œì‘ì¼}}", detail_data.get("ì‹œì‘ì¼", ""))
    html = html.replace("{{ì¢…ë£Œì¼}}", detail_data.get("ì¢…ë£Œì¼", ""))
    html = html.replace("{{ì§€ì ëª…}}", detail_data.get("ì§€ì ëª…", ""))
    html = html.replace("{{event_id}}", detail_data.get("id", ""))
    html = html.replace("{{ìƒì„¸ ë§í¬}}", detail_data.get("ìƒì„¸ ë§í¬", "#"))

    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ HTML ìƒì„± (ì´ë¯¸ì§€ ì €ì¥ ì—†ì´ URLë§Œ ì‚¬ìš©)
    product_html = ""
    for p in detail_data["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
        product_html += f"""
        <div class='product'>
          <img
            src="{p['ì´ë¯¸ì§€']}"
            alt="{p['ì œí’ˆëª…']} í–‰ì‚¬ ì´ë¯¸ì§€"
            loading="lazy" width="800" height="800"
          />
          <h3 class='name'>{p['ì œí’ˆëª…']}</h3>
          <p class='price'>{p['ê°€ê²©']}</p>
        </div>
        """

    html = html.replace("{{ìƒí’ˆ ëª©ë¡}}", product_html)

    # pages í´ë”ì— SEO ì¹œí™”ì ì¸ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    output_dir = os.path.join(BASE_DIR, "../outlet-web/pages")
    os.makedirs(output_dir, exist_ok=True)
    
    # ì§€ì ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜
    branch_mapping = {
        "ì†¡ë„": "songdo",
        "ê¹€í¬": "gimpo", 
        "ìŠ¤í˜ì´ìŠ¤ì›": "spaceone"
    }
    
    branch = detail_data.get("ì§€ì ëª…", "ì†¡ë„")
    branch_en = branch_mapping.get(branch, "songdo")
    
    # ì œëª©ì„ URL ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜
    def slugify(text):
        import re
        # í•œê¸€ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        text = text.replace(' ', '-')
        text = re.sub(r'-+', '-', text)
        text = text.strip('-')
        return text.lower()
    
    title_slug = slugify(detail_data["ì œëª©"])
    
    # ìƒˆ URL ê²½ë¡œ ìƒì„±
    url_path = f"{branch_en}/{title_slug}"
    filename = f"{branch_en}-{title_slug}.html"
    
    # filename ë³€ìˆ˜ë¥¼ í…œí”Œë¦¿ì— ì¶”ê°€
    html = html.replace("{{filename}}", filename)
    
    # íŒŒì¼ëª… ìƒì„± (pages í´ë” ì•ˆì— ì €ì¥)
    filename_html = os.path.join(output_dir, filename)
    
    # HTML íŒŒì¼ ì €ì¥
    with open(filename_html, "w", encoding="utf-8") as f:
        f.write(html)
    
    # URL ë§¤í•‘ì— ì¶”ê°€ (ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©)
    event_id = detail_data.get("id", "")
    if event_id and 'url_mapping' in globals():
        url_mapping[event_id] = filename
    
    print(f"âœ” SEO ì¹œí™”ì ì¸ HTML ìƒì„± ì™„ë£Œ: {url_path}")
    
    return url_path

def generate_sitemap(pages_dir, base_url, output_path):
    urls = []
    # â”€â”€ â‘  ë£¨íŠ¸ í˜ì´ì§€(https://discounts.deluxo.co.kr/) ì¶”ê°€
    today = datetime.today().strftime('%Y-%m-%d')
    # base_urlì´ "https://discounts.deluxo.co.kr/pages" ë¼ë©´
    site_root = base_url.rsplit("/", 1)[0] + "/"
    urls.append((site_root, today))
    # â”€â”€ â‘¡ (ì„ íƒ) privacy.html ê°™ì€ ì •ì  í˜ì´ì§€ ì¶”ê°€
    urls.append((site_root + "privacy.html", today))
    
    # ìƒˆë¡œìš´ SEO ì¹œí™”ì ì¸ URL êµ¬ì¡°ì˜ íŒŒì¼ë“¤ ì²˜ë¦¬
    for filename in os.listdir(pages_dir):
        if filename.endswith(".html") and '-' in filename and not filename.startswith('index'):
            filepath = os.path.join(pages_dir, filename)
            lastmod = datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d')
            
            # íŒŒì¼ëª…ì„ URL ê²½ë¡œë¡œ ë³€í™˜
            name_without_ext = filename.replace('.html', '')
            if name_without_ext.startswith('songdo-'):
                url_path = name_without_ext.replace('songdo-', 'songdo/')
            elif name_without_ext.startswith('gimpo-'):
                url_path = name_without_ext.replace('gimpo-', 'gimpo/')
            elif name_without_ext.startswith('spaceone-'):
                url_path = name_without_ext.replace('spaceone-', 'spaceone/')
            else:
                continue
            
            url = f"{base_url}/{url_path}"
            urls.append((url, lastmod))

    sitemap = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    for url, lastmod in urls:
        sitemap.append("  <url>")
        sitemap.append(f"    <loc>{url}</loc>")
        sitemap.append(f"    <lastmod>{lastmod}</lastmod>")
        sitemap.append("    <changefreq>daily</changefreq>")
        prio = "1.0" if url.rstrip("/") == site_root.rstrip("/") else "0.8"
        sitemap.append(f"    <priority>{prio}</priority>")
        sitemap.append("  </url>")
    sitemap.append('</urlset>')

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(sitemap))
    print(f"âœ” ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap.xml ìƒì„± ì™„ë£Œ: {output_path}")

def generate_index(pages_dir, index_path):
    import os
    from datetime import datetime
    from bs4 import BeautifulSoup

    links = []
    for fn in sorted(os.listdir(pages_dir)):
        if not (fn.startswith("event-") and fn.endswith(".html")):
            continue
        filepath = os.path.join(pages_dir, fn)
        # â‘  íŒŒì¼ ì—´ì–´ì„œ ì œëª©(<h1> ë˜ëŠ” í…œí”Œë¦¿ êµ¬ì¡°ì— ë§ëŠ” ìš”ì†Œ) íŒŒì‹±
        with open(filepath, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            # ì˜ˆì‹œ: ìƒì„¸í˜ì´ì§€ í…œí”Œë¦¿ì—ì„œ <h1>íƒœê·¸ì— ì´ë²¤íŠ¸ ì œëª©ì´ ìˆë‹¤ë©´
            title_el = soup.select_one("h1") or soup.select_one("section.fixArea h2")
            title = title_el.get_text(strip=True) if title_el else fn.replace("event-", "").replace(".html","")
        url = f"pages/{fn}"
        links.append((title, url))

    # ìƒìœ„ 5ê°œì™€ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë¶„ë¦¬
    preview_links = links[:5]
    full_links    = links

    # <li> ë¬¸ìì—´ë¡œ ë³€í™˜
    preview_lis = "\n".join(f'    <li><a href="{u}">{t}</a></li>' for t,u in preview_links)
    full_lis    = "\n".join(f'    <li><a href="{u}">{t}</a></li>' for t,u in full_links)

    # í…œí”Œë¦¿ ë¡œë“œ
    tpl_path = os.path.join(os.path.dirname(__file__), "index.tpl.html")
    tpl = open(tpl_path, encoding="utf-8").read()

    # ìë¦¬í‘œì‹œì ì¹˜í™˜
    html = tpl.replace("{{PREVIEW_LINKS}}", preview_lis)
    html = html.replace("{{EVENT_LINKS}}", full_lis)

    # ê²°ê³¼ ì €ì¥
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"âœ” index.html ìƒì„± ì™„ë£Œ: {index_path}")


# --- Google Sheets ì—…ë¡œë“œ
def upload_to_google_sheet(sheet_title, sheet_name, new_rows):
    today_str = datetime.today().strftime('%Y-%m-%d')
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    CREDENTIAL_PATH = os.path.join(BASE_DIR, "credentials.json")
    creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIAL_PATH, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open(sheet_title)
    try:
        worksheet = spreadsheet.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="1000", cols="20")

    headers = ["ì œëª©", "ê¸°ê°„", "ìƒì„¸ ì œëª©", "ìƒì„¸ ê¸°ê°„", "ì¸ë„¤ì¼", "ìƒì„¸ ë§í¬", "í˜œíƒ ì„¤ëª…",
               "ë¸Œëœë“œ", "ì œí’ˆëª…", "ê°€ê²©", "ì´ë¯¸ì§€", "ì—…ë°ì´íŠ¸ ë‚ ì§œ", "event_id"]
    try:
        existing_data = worksheet.get_all_values()
        if existing_data and existing_data[0] == headers:
            existing_data = existing_data[1:]
    except:
        existing_data = []

    existing_links = {row[5] for row in existing_data if len(row) >= 6}
    filtered_new_rows = [row for row in new_rows if len(row) >= 6 and row[5] not in existing_links]
    print(f"âœ¨ [{sheet_name}] ìƒˆë¡œ ì¶”ê°€í•  í•­ëª© ìˆ˜: {len(filtered_new_rows)}ê°œ")

    if not filtered_new_rows:
        print(f"âœ… [{sheet_name}] ì¶”ê°€í•  ë°ì´í„° ì—†ìŒ.")
        return

    all_data = [headers] + filtered_new_rows + existing_data
    worksheet.clear()
    worksheet.update('A1', all_data)
    print(f"âœ… [{sheet_name}] ì´ {len(all_data)-1}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
    print(f"ğŸ”— ì‹œíŠ¸ ë§í¬: https://docs.google.com/spreadsheets/d/{spreadsheet.id}/edit")

# --- ì•„ìš¸ë › í¬ë¡¤ë§
def crawl_outlet(branchCd, outletName, sheet_name):
    driver = setup_driver()
    new_rows = []
    for page in range(1, 5):
        print(f"[{sheet_name}] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        events = fetch_event_list(driver, branchCd, page)
        for event in events:
            title_tag = event.select_one(".info_tit")
            period_tag = event.select_one(".info_txt")
            img_tag = event.select_one("img")
            link_tag = event.select_one("a")
            title = title_tag.get_text(separator=" ", strip=True) if title_tag else ""
            period = period_tag.get_text(strip=True) if period_tag else ""
            image_url = img_tag["src"] if img_tag else ""
            detail_url = "https://www.ehyundai.com" + link_tag["href"] if link_tag else ""
            detail = fetch_event_detail(driver, detail_url)
            thumbnail_id = image_url.split("/")[-1].split(".")[0][-12:]
            event_id = thumbnail_id
            detail_data = {
                "id":       event_id,
                "ì œëª©":     title,
                "ê¸°ê°„":     period,
                "ìƒì„¸ ì œëª©": detail["ìƒì„¸ ì œëª©"],
                "ìƒì„¸ ê¸°ê°„": detail["ìƒì„¸ ê¸°ê°„"],
                "ì‹œì‘ì¼":   detail["ì‹œì‘ì¼"],    # â† ISO ì‹œì‘ì¼
                "ì¢…ë£Œì¼":   detail["ì¢…ë£Œì¼"],    # â† ISO ì¢…ë£Œì¼
                "ì§€ì ëª…":   outletName,          # â† ì§€ì ëª… ì¶”ê°€
                "ì¸ë„¤ì¼":   image_url,
                "ìƒì„¸ ë§í¬": detail_url,
                "í˜œíƒ ì„¤ëª…":" / ".join(detail["í…ìŠ¤íŠ¸ ì„¤ëª…"]),
                "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]
            }
            url_path = generate_html(detail_data, event_id)
            base_info = [title, period, detail["ìƒì„¸ ì œëª©"], detail["ìƒì„¸ ê¸°ê°„"],
                         image_url, detail_url, detail_data["í˜œíƒ ì„¤ëª…"]]
            if detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                for p in detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                    row = base_info + [
                        p["ë¸Œëœë“œ"], p["ì œí’ˆëª…"], p["ê°€ê²©"], p["ì´ë¯¸ì§€"],
                        datetime.today().strftime('%Y-%m-%d'), event_id]
                    new_rows.append(row)
            else:
                new_rows.append(base_info + ["", "", "", "",
                                 datetime.today().strftime('%Y-%m-%d'), event_id])
    driver.quit()
    upload_to_google_sheet("outlet-data", sheet_name, new_rows)

# --- ë©”ì¸ ì‹¤í–‰
def main():
    OUTLET_TARGETS = [
        ("B00174000", "ì†¡ë„",     "Sheet1"),
        ("B00172000", "ê¹€í¬",     "Sheet2"),
        ("B00178000", "ìŠ¤í˜ì´ìŠ¤ì›","Sheet3"),
    ]

    for branchCd, outletName, sheet_name in OUTLET_TARGETS:
        crawl_outlet(branchCd, outletName, sheet_name)
    
    # URL ë§¤í•‘ JSON íŒŒì¼ ìƒì„±
    import json
    mapping_path = os.path.join(os.path.dirname(__file__), "../outlet-web/url-mapping.json")
    with open(mapping_path, "w", encoding="utf-8") as f:
        json.dump(url_mapping, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ URL ë§¤í•‘ íŒŒì¼ ìƒì„±: {len(url_mapping)}ê°œ í•­ëª©")

    # âœ… ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap.xml ìƒì„±
    generate_sitemap(
        pages_dir=os.path.join(os.path.dirname(__file__), "../outlet-web/pages"),
        base_url="https://discounts.deluxo.co.kr/pages",
        output_path=os.path.join(os.path.dirname(__file__), "../outlet-web/sitemap.xml")
    )

    # âœ… index.html (ì •ì  ì´ë²¤íŠ¸ ë§í¬ ëª©ë¡) ìƒì„±
    generate_index(
        pages_dir=os.path.join(os.path.dirname(__file__), "../outlet-web/pages"),
        index_path=os.path.join(os.path.dirname(__file__), "../outlet-web/index.html")
    )

    print("\nğŸ‰ ì „ì²´ ì•„ìš¸ë › í¬ë¡¤ë§ ë° ì €ì¥ + ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap ìƒì„± ì™„ë£Œ!")
    print("ğŸ”— ìƒˆë¡œìš´ URL êµ¬ì¡°: discounts.deluxo.co.kr/pages/{ì§€ì ëª…}-{ì œëª©}.html")

if __name__ == "__main__":
    main()