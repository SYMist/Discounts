import time
import os
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ê²½ë¡œ/í™˜ê²½ì„¤ì • (ê¸°ë³¸ê°’ì€ ê¸°ì¡´ ê²½ë¡œ ìœ ì§€)
CFG = None

def _get_config():
    """í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•´ ê²½ë¡œë¥¼ ì¬ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.
    ê¸°ë³¸ê°’ì€ í˜„ì¬ ì½”ë“œê°€ ì‚¬ìš©í•˜ë˜ ìƒëŒ€ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    
    ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ë³€ìˆ˜
    - OUTLET_TEMPLATE_PATH
    - OUTLET_INDEX_TEMPLATE_PATH
    - OUTLET_PAGES_DIR
    - OUTLET_MAPPING_PATH
    - OUTLET_SITE_BASE_URL
    - OUTLET_SITEMAP_PATH
    - OUTLET_INDEX_OUTPUT_PATH
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return {
        "TEMPLATE_PATH": os.environ.get(
            "OUTLET_TEMPLATE_PATH",
            os.path.join(base_dir, "templates", "template.html"),
        ),
        "INDEX_TEMPLATE_PATH": os.environ.get(
            "OUTLET_INDEX_TEMPLATE_PATH",
            os.path.join(base_dir, "templates", "index.tpl.html"),
        ),
        "PAGES_DIR": os.environ.get(
            "OUTLET_PAGES_DIR",
            os.path.join(base_dir, "../web/public/pages"),
        ),
        "MAPPING_PATH": os.environ.get(
            "OUTLET_MAPPING_PATH",
            os.path.join(base_dir, "../web/public/url-mapping.json"),
        ),
        "SITE_BASE_URL": os.environ.get(
            "OUTLET_SITE_BASE_URL",
            "https://discounts.deluxo.co.kr",
        ),
        "SITEMAP_PATH": os.environ.get(
            "OUTLET_SITEMAP_PATH",
            os.path.join(base_dir, "../web/public/sitemap.xml"),
        ),
        "INDEX_OUTPUT_PATH": os.environ.get(
            "OUTLET_INDEX_OUTPUT_PATH",
            os.path.join(base_dir, "../web/public/index.html"),
        ),
    }

# --- ì „ì—­ ë³€ìˆ˜ (mainì—ì„œ ì´ˆê¸°í™”)
url_mapping = {}

# --- ìœ í‹¸: ë‚ ì§œ í¬ë§· ë³€í™˜(YYYYMMDDHHMMSS -> M.D)
def _fmt_md(yyyymmddhhmmss: str) -> str:
    try:
        if not yyyymmddhhmmss:
            return ""
        s = yyyymmddhhmmss.strip()
        y = int(s[0:4]); m = int(s[4:6]); d = int(s[6:8])
        return f"{m}.{d}"
    except Exception:
        return ""

def parse_period(period_text):
    # 1) ì¤„ë°”ê¿ˆ ì œê±°
    clean = period_text.replace("\n", "").replace("\r", "")
    # 2) ê´„í˜¸ ì•ˆ ì„¤ëª… ëª¨ë‘ ì œê±°
    clean = re.sub(r"\([^)]*\)", "", clean)
    # 3) ê³µë°±(ìŠ¤í˜ì´ìŠ¤) ì „ë¶€ ì œê±°
    clean = clean.replace(" ", "")
    # 4) '~' ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    parts = clean.split("~")
    if len(parts) != 2:
        return "", ""
    # 5) ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜
    def to_iso(s):
        if "." not in s:
            return ""
        m, d = s.split(".")
        return f"2025-{m.zfill(2)}-{d.zfill(2)}"
    return to_iso(parts[0]), to_iso(parts[1])

# --- WebDriver ì„¤ì •
def setup_driver():
    options = Options()
    options.add_argument("--window-size=1920,1080")
    # Optional headless mode for non-interactive environments
    import os as _os
    if _os.environ.get("OUTLET_HEADLESS", "").lower() in ("1", "true", "yes"): 
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    return driver

# --- ê°€ê²© í…ìŠ¤íŠ¸ ì²˜ë¦¬
def process_price_text(price_text):
    if "ì •ìƒê°€" in price_text and "íŒë§¤ê°€" in price_text:
        try:
            parts = price_text.split("íŒë§¤ê°€")
            original_price = parts[0].strip()
            sale_price = parts[1].strip()
            return f"<s>{original_price}</s> íŒë§¤ê°€ {sale_price}"
        except:
            return price_text
    else:
        return price_text

# --- í–‰ì‚¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_list(driver, branchCd, page):
    """í–‰ì‚¬ ëª©ë¡ í˜ì´ì§• ìˆ˜ì§‘.
    - ê¸°ë³¸: HTTP(AJAX) ìš”ì²­ìœ¼ë¡œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ì§‘
    - í´ë°±: Selenium + getContents ë˜ëŠ” ì²« í˜ì´ì§€ íŒŒì‹±
    ë°˜í™˜: 
      - HTTP ëª¨ë“œ: dict ë¦¬ìŠ¤íŠ¸ [{title, period, image, link}]
      - í´ë¦­/í´ë°±: BeautifulSoup li ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
    """
    import os as _os, re, requests
    mode = _os.environ.get("OUTLET_LISTING_MODE", "http").lower()

    list_url = f"https://www.ehyundai.com/newPortal/SN/SN_0101000.do?branchCd={branchCd}&SN=1"

    if mode == "http":
        try:
            html = requests.get(list_url, timeout=15).text
            m = re.search(r"var\s+curtMblDmCd\s*=\s*'([^']+)'", html)
            if not m:
                raise RuntimeError("mblDmCd íŒŒì‹± ì‹¤íŒ¨")
            mbl = m.group(1)
            # typeCd '01' = ì´ë²¤íŠ¸
            api = "https://www.ehyundai.com/newPortal/SN/GetCmsContentsAJX.do"
            params = {
                'apiID': 'ifAppHdcms012',
                'param': f"mblDmCd={mbl}&evntCrdTypeCd=01&pageSize=9&page={page}",
            }
            js = requests.get(api, params=params, timeout=15).json()
            items = js.get('result', {}).get('items', [])
            evts = []
            for it in items:
                # ì¹´í…Œê³ ë¦¬ íŒë‹¨
                t = (it.get('evntCrdTypeCd') or {}).get('value') or '01'
                if t == '02':
                    category = 'gift'
                elif t == '03':
                    category = 'culture'
                elif t == '04':
                    category = 'special'
                else:
                    category = 'event'
                img = it.get('imgPath2') or ''
                if img:
                    img = ("https://imgprism.ehyundai.com/" + img).replace("https://apiprism.ehyundai.com/", "")
                # ê¸°ê°„ ë¬¸ìì—´ êµ¬ì„±
                stCd = (it.get('expsEvntStartGbcd') or {}).get('value')
                enCd = (it.get('expsEvntEndGbcd') or {}).get('value')
                st = it.get('expsEvntStartTxt') if stCd == '02' else _fmt_md(it.get('expsEvntStartDt') or '')
                en = it.get('expsEvntEndTxt') if enCd == '02' else _fmt_md(it.get('expsEvntEndDt') or '')
                period = (st or '') + (" ~ " if (st or en) and en else "") + (en or '')
                link = f"https://www.ehyundai.com/newPortal/SN/SN_0201000.do?evntCrdCd={it.get('evntCrdCd')}&category={category}&page={page}&branchCd={branchCd}"
                evts.append({
                    'title': it.get('evntCrdNm') or '',
                    'period': period,
                    'image': img,
                    'link': link,
                })
            return evts
        except Exception as e:
            print(f"âš ï¸ HTTP ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}. í´ë¦­ ëª¨ë“œë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            # í´ë°±ìœ¼ë¡œ ê³„ì† ì§„í–‰

    # â”€â”€ í´ë¦­/í´ë°± ëª¨ë“œ
    driver.get(list_url)
    # Wait for page scripts to load (getContents function) then load requested page
    try:
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return typeof getContents === 'function'")
        )
        try:
            driver.execute_script(f"getContents('01', {page}, 0);")
        except Exception as e:
            print(f"âš ï¸ getContents í˜¸ì¶œ ì‹¤íŒ¨: {e}. ì²« í˜ì´ì§€ ê·¸ëŒ€ë¡œ íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    except Exception:
        # ìŠ¤í¬ë¦½íŠ¸ ë¯¸íƒ‘ì¬ ì‹œ ì²« í˜ì´ì§€ íŒŒì‹±ë§Œ
        pass
    # Wait for event list elements to be present
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#eventList > li"))
        )
    except Exception:
        time.sleep(2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("#eventList > li")

# --- í–‰ì‚¬ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_detail(driver, url):
    try:
        driver.get(url)
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # â‘  ì œëª© ê°€ì ¸ì˜¤ê¸° (fixAreas â†’ fixArea)
        title_el = soup.select_one("section.fixArea h2")
        title_text = title_el.text.strip() if title_el else ""

        # â‘¡ ê¸°ê°„ ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
        period_el = soup.select_one("table.info td")
        period_text = period_el.text.strip() if period_el else ""

        # â‘¢ ISO í¬ë§· ë‚ ì§œ íŒŒì‹± & í…ìŠ¤íŠ¸ ì„¤ëª… ìˆ˜ì§‘
        start_iso, end_iso = parse_period(period_text)

        noimg_list = [
            f"{r.find('th').text.strip()}: {r.find('td').text.strip()}"
            for r in soup.select("article.noImgProduct tr")
            if r.find('th') and r.find('td')
        ]

        products = []
        for p in soup.select("article.twoProduct figure"):
            # 1) ì¤„ë°”ê¿ˆ ì œê±°: ê³µë°± í•˜ë‚˜ë¡œ í†µì¼
            brand_text = (
                p.select_one(".p_brandNm").get_text(" ", strip=True)
                if p.select_one(".p_brandNm")
                else ""
            )
            name_text = (
                p.select_one(".p_productNm").get_text(" ", strip=True)
                if p.select_one(".p_productNm")
                else ""
            )

            # 2) í•´ì‹œíƒœê·¸ ì œê±°
            brand_text = " ".join(w for w in brand_text.split() if not w.startswith("#"))

            # 3) ë¶„ë¥˜ í† í° ì œê±°
            if brand_text.upper() in ("MEN", "WOMEN", "MEN/WOMEN"):
                brand_text = ""

            # 4) name_textê°€ ë¹„ì–´ìˆìœ¼ë©´ brand_text â†’ name_text
            if not name_text and brand_text:
                name_text, brand_text = brand_text, ""

            # 5) '[ë¸Œëœë“œ]' ë‘˜ëŸ¬ì‹¼ ê²½ìš° ëŒ€ê´„í˜¸ ì œê±°
            if re.fullmatch(r"\[[^\]]+\]", brand_text):
                brand_text = brand_text[1:-1].strip()

            # 6) brand_text ì—†ì„ ë•Œ '[ë¸Œëœë“œ]ì œí’ˆëª…' ë¶„ë¦¬
            if not brand_text:
                m = re.match(r"^\[([^\]]+)\]\s*(.+)$", name_text)
                if m:
                    brand_text = m.group(1).strip()
                    name_text = m.group(2).strip()

            # 7) ë³€í˜• ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°(ìŠ¬ë˜ì‹œ í¬í•¨) í•©ì¹˜ê¸°
            if brand_text and "/" in name_text:
                name_text = f"{brand_text} {name_text}"
                brand_text = ""

            # 8) í”„ë¡œëª¨ì…˜/ì¦ì • í•­ëª© ê±´ë„ˆë›°ê¸°
            if "ì¦ì •" in brand_text or "êµ¬ë§¤ì‹œ" in name_text or name_text.startswith("ã€Œ"):
                continue

            # 9) SKU ì½”ë“œë§Œ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if re.fullmatch(r"[A-Z0-9]+", name_text):
                continue

            # ê°€ê²© ë° ì´ë¯¸ì§€ URL
            price_tag = p.select_one(".p_productPrc")
            price_txt = price_tag.get_text(" ", strip=True) if price_tag else ""
            img_tag = p.select_one(".p_productImg")
            img_url = img_tag["src"] if img_tag else ""

            # ìµœì¢… ê³µë°± ì •ë¦¬
            name_text = " ".join(name_text.split())

            products.append({
                "ë¸Œëœë“œ": brand_text,
                "ì œí’ˆëª…": name_text,
                "ê°€ê²©": process_price_text(price_txt),
                "ì´ë¯¸ì§€": img_url
            })

        return {
            "ìƒì„¸ ì œëª©": title_text,
            "ìƒì„¸ ê¸°ê°„": period_text,
            "ì‹œì‘ì¼": start_iso,
            "ì¢…ë£Œì¼": end_iso,
            "í…ìŠ¤íŠ¸ ì„¤ëª…": noimg_list,
            "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": products
        }

    except Exception as e:
        print(f"âŒ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {"ìƒì„¸ ì œëª©": "", "ìƒì„¸ ê¸°ê°„": "", "ì‹œì‘ì¼":"", "ì¢…ë£Œì¼":"", "í…ìŠ¤íŠ¸ ì„¤ëª…": [], "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": []}

def fetch_event_detail_http(url):
    """Selenium ì—†ì´ HTTPë¡œ ìƒì„¸ í˜ì´ì§€ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        import requests as _req
        html = _req.get(url, timeout=15).text
        soup = BeautifulSoup(html, "html.parser")

        title_el = soup.select_one("section.fixArea h2")
        title_text = title_el.text.strip() if title_el else ""

        period_el = soup.select_one("table.info td")
        period_text = period_el.text.strip() if period_el else ""
        start_iso, end_iso = parse_period(period_text)

        noimg_list = [
            f"{r.find('th').text.strip()}: {r.find('td').text.strip()}"
            for r in soup.select("article.noImgProduct tr")
            if r.find('th') and r.find('td')
        ]

        products = []
        for p in soup.select("article.twoProduct figure"):
            brand_text = (
                p.select_one(".p_brandNm").get_text(" ", strip=True)
                if p.select_one(".p_brandNm")
                else ""
            )
            name_text = (
                p.select_one(".p_productNm").get_text(" ", strip=True)
                if p.select_one(".p_productNm")
                else ""
            )
            brand_text = " ".join(w for w in brand_text.split() if not w.startswith("#"))
            if brand_text.upper() in ("MEN", "WOMEN", "MEN/WOMEN"):
                brand_text = ""
            if not name_text and brand_text:
                name_text, brand_text = brand_text, ""
            if re.fullmatch(r"\[[^\]]+\]", brand_text):
                brand_text = brand_text[1:-1].strip()
            if not brand_text:
                m = re.match(r"^\[([^\]]+)\]\s*(.+)$", name_text)
                if m:
                    brand_text = m.group(1).strip()
                    name_text = m.group(2).strip()
            if brand_text and "/" in name_text:
                name_text = f"{brand_text} {name_text}"
                brand_text = ""
            if "ì¦ì •" in brand_text or "êµ¬ë§¤ì‹œ" in name_text or name_text.startswith("ã€Œ"):
                continue
            if re.fullmatch(r"[A-Z0-9]+", name_text):
                continue
            price_tag = p.select_one(".p_productPrc")
            price_txt = price_tag.get_text(" ", strip=True) if price_tag else ""
            img_tag = p.select_one(".p_productImg")
            img_url = img_tag["src"] if img_tag else ""
            name_text = " ".join(name_text.split())
            products.append({
                "ë¸Œëœë“œ": brand_text,
                "ì œí’ˆëª…": name_text,
                "ê°€ê²©": process_price_text(price_txt),
                "ì´ë¯¸ì§€": img_url
            })

        return {
            "ìƒì„¸ ì œëª©": title_text,
            "ìƒì„¸ ê¸°ê°„": period_text,
            "ì‹œì‘ì¼": start_iso,
            "ì¢…ë£Œì¼": end_iso,
            "í…ìŠ¤íŠ¸ ì„¤ëª…": noimg_list,
            "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": products
        }
    except Exception as e:
        print(f"âŒ ìƒì„¸í˜ì´ì§€(HTTP) í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {"ìƒì„¸ ì œëª©": "", "ìƒì„¸ ê¸°ê°„": "", "ì‹œì‘ì¼":"", "ì¢…ë£Œì¼":"", "í…ìŠ¤íŠ¸ ì„¤ëª…": [], "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": []}
        
# --- HTML í˜ì´ì§€ ìƒì„±
def generate_html(detail_data, event_id):
    global CFG
    # í…œí”Œë¦¿ ê²½ë¡œ: í™˜ê²½ì„¤ì • ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ê¸°ë³¸ê°’
    template_path = None
    if CFG and CFG.get("TEMPLATE_PATH"):
        template_path = CFG["TEMPLATE_PATH"]
    else:
        template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "templates", "template.html")

    with open(template_path, "r", encoding="utf-8") as f:
        template = f.read()

    # í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
    html = template
    # ì œëª©/ì„¤ëª… ì •ë¦¬: ê°œí–‰ ì œê±° ë° JSON-LDìš© ì´ìŠ¤ì¼€ì´í”„ ê°’ ì¶”ê°€
    import json as _json
    title_clean = detail_data["ì œëª©"].replace("\r", " ").replace("\n", " ").strip()
    desc_clean = detail_data["í˜œíƒ ì„¤ëª…"].replace("\r", " ").strip()
    html = html.replace("{{ì œëª©}}", title_clean)
    html = html.replace("{{ê¸°ê°„}}", detail_data["ê¸°ê°„"])
    html = html.replace("{{ìƒì„¸ ì œëª©}}", detail_data["ìƒì„¸ ì œëª©"])
    html = html.replace("{{ìƒì„¸ ê¸°ê°„}}", detail_data["ìƒì„¸ ê¸°ê°„"])
    html = html.replace("{{ì¸ë„¤ì¼}}", detail_data.get("ì¸ë„¤ì¼", ""))
    html = html.replace("{{í˜œíƒ ì„¤ëª…}}", desc_clean.replace("\n", "<br>"))
    # JSON-LDì— ì•ˆì „í•˜ê²Œ ì‚½ì…í•  ìˆ˜ ìˆë„ë¡ JSON ë¬¸ìì—´ë¡œ ì´ìŠ¤ì¼€ì´í”„ëœ ê°’ ì£¼ì…
    html = html.replace("{{ì œëª©_JSON}}", _json.dumps(title_clean, ensure_ascii=False))
    html = html.replace("{{í˜œíƒ ì„¤ëª…_JSON}}", _json.dumps(desc_clean, ensure_ascii=False))
    html = html.replace("{{ì—…ë°ì´íŠ¸ ë‚ ì§œ}}", datetime.today().strftime('%Y-%m-%d'))
    html = html.replace("{{ì‹œì‘ì¼}}", detail_data.get("ì‹œì‘ì¼", ""))
    html = html.replace("{{ì¢…ë£Œì¼}}", detail_data.get("ì¢…ë£Œì¼", ""))
    html = html.replace("{{ì§€ì ëª…}}", detail_data.get("ì§€ì ëª…", ""))
    html = html.replace("{{event_id}}", detail_data.get("id", ""))
    html = html.replace("{{ìƒì„¸ ë§í¬}}", detail_data.get("ìƒì„¸ ë§í¬", "#"))

    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ HTML ìƒì„± (ì´ë¯¸ì§€ ì €ì¥ ì—†ì´ URLë§Œ ì‚¬ìš©)
    product_html = ""
    for p in detail_data["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
        product_html += f"""
        <div class='product'>
          <img
            src="{p['ì´ë¯¸ì§€']}"
            alt="{p['ì œí’ˆëª…']} í–‰ì‚¬ ì´ë¯¸ì§€"
            loading="lazy" width="800" height="800"
          />
          <h3 class='name'>{p['ì œí’ˆëª…']}</h3>
          <p class='price'>{p['ê°€ê²©']}</p>
        </div>
        """

    html = html.replace("{{ìƒí’ˆ ëª©ë¡}}", product_html)

    # pages í´ë”ì— SEO ì¹œí™”ì ì¸ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    # ì¶œë ¥ ë””ë ‰í† ë¦¬: í™˜ê²½ì„¤ì • ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ê¸°ë³¸ê°’
    output_dir = None
    if CFG and CFG.get("PAGES_DIR"):
        output_dir = CFG["PAGES_DIR"]
    else:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "../web/public/pages")
    os.makedirs(output_dir, exist_ok=True)
    
    # ì§€ì ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜
    branch_mapping = {
        "ì†¡ë„": "songdo",
        "ê¹€í¬": "gimpo", 
        "ìŠ¤í˜ì´ìŠ¤ì›": "spaceone"
    }
    
    branch = detail_data.get("ì§€ì ëª…", "ì†¡ë„")
    branch_en = branch_mapping.get(branch, "songdo")
    
    # ì œëª©ì„ URL ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜
    def slugify(text):
        import re
        # í•œê¸€ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        text = text.replace(' ', '-')
        text = re.sub(r'-+', '-', text)
        text = text.strip('-')
        return text.lower()
    
    title_slug = slugify(detail_data["ì œëª©"])
    
    # ìƒˆ URL ê²½ë¡œ ìƒì„±
    url_path = f"{branch_en}/{title_slug}"
    filename = f"{branch_en}-{title_slug}.html"
    
    # í…œí”Œë¦¿ì— ê²½ë¡œ ë³€ìˆ˜ ì£¼ì…
    html = html.replace("{{filename}}", filename)
    # ì‚¬ì´íŠ¸ ê¸°ë³¸ URL: í™˜ê²½ì„¤ì • ì‚¬ìš©
    site_base = (CFG or {}).get("SITE_BASE_URL", "https://discounts.deluxo.co.kr")
    pretty_path = f"{site_base.rstrip('/')}/{url_path}"
    html = html.replace("{{pretty_path}}", pretty_path)
    html = html.replace("{{site_base}}", site_base.rstrip('/'))
    html = html.replace("{{branch_en}}", branch_en)

    # ê´€ë ¨ í–‰ì‚¬(ê°™ì€ ì§€ì ) 3~5ê°œ ìƒì„±
    # ê¸°ì¤€: ë™ì¼ ë¸Œëœì¹˜ íŒŒì¼ë“¤ ì¤‘ ìµœê·¼ ìˆ˜ì • ìˆœ (í˜„ì¬ íŒŒì¼ ì œì™¸)
    related_html = []
    try:
        import os as _os
        from bs4 import BeautifulSoup as _BS
        candidates = []
        for fn in _os.listdir(output_dir):
            if not fn.endswith('.html'):
                continue
            if not fn.startswith(f"{branch_en}-"):
                continue
            if fn == filename:
                continue
            path = _os.path.join(output_dir, fn)
            mtime = _os.path.getmtime(path)
            # íƒ€ì´í‹€ ì¶”ì¶œ
            title_text = None
            try:
                with open(path, 'r', encoding='utf-8') as rf:
                    soup = _BS(rf, 'html.parser')
                    h1 = soup.find('h1')
                    if h1 and h1.get_text(strip=True):
                        title_text = h1.get_text(strip=True)
            except Exception:
                title_text = None
            candidates.append((mtime, fn, title_text))
        # ìµœì‹ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ
        candidates.sort(key=lambda x: -x[0])
        for _, fn, title_text in candidates[:5]:
            slug = fn[len(branch_en)+1:-5]
            pretty = f"/{branch_en}/{slug}"
            title_display = title_text or slug
            related_html.append(f"<li><a href=\"{pretty}\">{title_display}</a></li>")
    except Exception:
        pass
    html = html.replace("{{ê´€ë ¨ í–‰ì‚¬}}", "\n".join(related_html))
    
    # íŒŒì¼ëª… ìƒì„± (pages í´ë” ì•ˆì— ì €ì¥)
    filename_html = os.path.join(output_dir, filename)
    
    # HTML íŒŒì¼ ì €ì¥
    with open(filename_html, "w", encoding="utf-8") as f:
        f.write(html)
    
    # URL ë§¤í•‘ì— ì¶”ê°€ (ê°œì„ ëœ í¬ê´„ì  ë°©ì‹)
    event_id = detail_data.get("id", "")
    if event_id and 'url_mapping' in globals():
        mappings_count = add_comprehensive_mapping(event_id, filename)
        print(f"  ğŸ“Œ {mappings_count}ê°œ ë³€í˜• ë§¤í•‘ ì¶”ê°€: {event_id} â†’ {filename}")
    
    print(f"âœ” SEO ì¹œí™”ì ì¸ HTML ìƒì„± ì™„ë£Œ: {url_path}")
    
    return url_path

def add_comprehensive_mapping(event_id, filename):
    """ëª¨ë“  ê°€ëŠ¥í•œ event_id ë³€í˜•ë“¤ì„ ë§¤í•‘ì— ì¶”ê°€í•˜ì—¬ ê·¼ë³¸ì ìœ¼ë¡œ ë§í¬ ë¬¸ì œ í•´ê²°"""
    global url_mapping
    
    if not event_id:
        return 0
    
    mappings_added = 0
    
    # 1. ê¸°ë³¸ event_id ë§¤í•‘ ì¶”ê°€
    url_mapping[event_id] = filename
    mappings_added += 1
    
    # 2. _02 íŒ¨í„´ ì²˜ë¦¬
    if event_id.endswith('_02'):
        # _02ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ IDë„ ì¶”ê°€
        base_id = event_id[:-3]
        if base_id not in url_mapping:
            url_mapping[base_id] = filename
            mappings_added += 1
    else:
        # ê¸°ë³¸ IDë©´ _02 ë³€í˜•ë„ ì¶”ê°€
        extended_id = event_id + '_02'
        if extended_id not in url_mapping:
            url_mapping[extended_id] = filename
            mappings_added += 1
    
    # 3. _03, _04 ë“± ì¶”ê°€ ë³€í˜• ì˜ˆë°©ì  ì¶”ê°€
    if not any(event_id.endswith(suffix) for suffix in ['_02', '_03', '_04']):
        for suffix in ['_03', '_04']:
            variant_id = event_id + suffix
            if variant_id not in url_mapping:
                url_mapping[variant_id] = filename
                mappings_added += 1
    
    # 4. ê¸¸ì´ë³„ ë³€í˜• ì²˜ë¦¬ (12ìë¦¬ â†” 9ìë¦¬)
    if len(event_id) == 12 and not any(event_id.endswith(suffix) for suffix in ['_02', '_03', '_04']):
        # 12ìë¦¬ë©´ 9ìë¦¬ ë³€í˜•ë„ ì¶”ê°€
        short_id = event_id[:9]
        for suffix in ['', '_02', '_03', '_04']:
            short_variant = short_id + suffix
            if short_variant not in url_mapping:
                url_mapping[short_variant] = filename
                mappings_added += 1
    
    return mappings_added

def generate_sitemap(pages_dir, base_url, output_path):
    urls = []
    # â”€â”€ â‘  ë£¨íŠ¸ í˜ì´ì§€(https://discounts.deluxo.co.kr/) ì¶”ê°€
    today = datetime.today().strftime('%Y-%m-%d')
    # base_urlì€ "https://discounts.deluxo.co.kr" ê¸°ì¤€
    site_root = base_url.rstrip('/') + '/'
    urls.append((site_root, today))
    # â”€â”€ â‘¡ (ì„ íƒ) privacy.html ê°™ì€ ì •ì  í˜ì´ì§€ ì¶”ê°€
    urls.append((site_root + "privacy.html", today))
    
    # ìƒˆë¡œìš´ SEO ì¹œí™”ì ì¸ URL êµ¬ì¡°ì˜ íŒŒì¼ë“¤ ì²˜ë¦¬ (í”„ë¦¬í‹° URL ì‚¬ìš©)
    for filename in os.listdir(pages_dir):
        if filename.endswith(".html") and '-' in filename and not filename.startswith('index'):
            filepath = os.path.join(pages_dir, filename)
            lastmod = datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d')
            name_without_ext = filename[:-5]  # strip .html
            if name_without_ext.startswith('songdo-'):
                url_path = name_without_ext.replace('songdo-', 'songdo/')
            elif name_without_ext.startswith('gimpo-'):
                url_path = name_without_ext.replace('gimpo-', 'gimpo/')
            elif name_without_ext.startswith('spaceone-'):
                url_path = name_without_ext.replace('spaceone-', 'spaceone/')
            else:
                continue
            url = f"{base_url.rstrip('/')}/{url_path}"
            urls.append((url, lastmod))

    # ì´ë²¤íŠ¸ í—ˆë¸Œ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ í¬í•¨
    events_dir = os.path.abspath(os.path.join(pages_dir, '..', 'events'))
    if os.path.isdir(events_dir):
        for fn in os.listdir(events_dir):
            if not fn.endswith('.html'):
                continue
            fp = os.path.join(events_dir, fn)
            lastmod = datetime.fromtimestamp(os.path.getmtime(fp)).strftime('%Y-%m-%d')
            url = f"{base_url.rstrip('/')}/events/{fn}"
            if fn == 'index.html':
                url = f"{base_url.rstrip('/')}/events/"
            urls.append((url, lastmod))

    sitemap = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    for url, lastmod in urls:
        sitemap.append("  <url>")
        sitemap.append(f"    <loc>{url}</loc>")
        sitemap.append(f"    <lastmod>{lastmod}</lastmod>")
        sitemap.append("    <changefreq>daily</changefreq>")
        prio = "1.0" if url.rstrip("/") == site_root.rstrip("/") else "0.8"
        sitemap.append(f"    <priority>{prio}</priority>")
        sitemap.append("  </url>")
    sitemap.append('</urlset>')

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(sitemap))
    print(f"âœ” ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap.xml ìƒì„± ì™„ë£Œ: {output_path}")

def generate_index(pages_dir, index_path):
    import os
    from datetime import datetime
    from bs4 import BeautifulSoup

    links = []
    for fn in sorted(os.listdir(pages_dir)):
        if not (fn.startswith("event-") and fn.endswith(".html")):
            continue
        filepath = os.path.join(pages_dir, fn)
        # â‘  íŒŒì¼ ì—´ì–´ì„œ ì œëª©(<h1> ë˜ëŠ” í…œí”Œë¦¿ êµ¬ì¡°ì— ë§ëŠ” ìš”ì†Œ) íŒŒì‹±
        with open(filepath, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            # ì˜ˆì‹œ: ìƒì„¸í˜ì´ì§€ í…œí”Œë¦¿ì—ì„œ <h1>íƒœê·¸ì— ì´ë²¤íŠ¸ ì œëª©ì´ ìˆë‹¤ë©´
            title_el = soup.select_one("h1") or soup.select_one("section.fixArea h2")
            title = title_el.get_text(strip=True) if title_el else fn.replace("event-", "").replace(".html","")
        url = f"pages/{fn}"
        links.append((title, url))

    # ìƒìœ„ 5ê°œì™€ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë¶„ë¦¬
    preview_links = links[:5]
    full_links    = links

    # <li> ë¬¸ìì—´ë¡œ ë³€í™˜
    preview_lis = "\n".join(f'    <li><a href="{u}">{t}</a></li>' for t,u in preview_links)
    full_lis    = "\n".join(f'    <li><a href="{u}">{t}</a></li>' for t,u in full_links)

    # í…œí”Œë¦¿ ë¡œë“œ
    # ì¸ë±ìŠ¤ í…œí”Œë¦¿ ê²½ë¡œ: í™˜ê²½ì„¤ì • ìš°ì„ 
    global CFG
    tpl_path = None
    if CFG and CFG.get("INDEX_TEMPLATE_PATH"):
        tpl_path = CFG["INDEX_TEMPLATE_PATH"]
    else:
        tpl_path = os.path.join(os.path.dirname(__file__), "index.tpl.html")
    tpl = open(tpl_path, encoding="utf-8").read()

    # ìë¦¬í‘œì‹œì ì¹˜í™˜
    html = tpl.replace("{{PREVIEW_LINKS}}", preview_lis)
    html = html.replace("{{EVENT_LINKS}}", full_lis)

    # ê²°ê³¼ ì €ì¥
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"âœ” index.html ìƒì„± ì™„ë£Œ: {index_path}")


# --- Google Sheets ì—…ë¡œë“œ
def upload_to_google_sheet(sheet_title, sheet_name, new_rows):
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    today_str = datetime.today().strftime('%Y-%m-%d')
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    # ìê²©ì¦ëª… ê²½ë¡œ: ENV ìš°ì„  â†’ ì‹ ê·œ ê¸°ë³¸ê²½ë¡œ(apps/crawler/credentials.json) â†’ ë ˆê±°ì‹œ(outlet-crawler/credentials.json)
    cred_env = os.environ.get("OUTLET_CREDENTIALS_PATH")
    if cred_env and os.path.exists(cred_env):
        credential_path = cred_env
    else:
        new_default = os.path.join(BASE_DIR, "credentials.json")
        legacy_default = os.path.join(BASE_DIR, "..", "..", "outlet-crawler", "credentials.json")
        if os.path.exists(new_default):
            credential_path = new_default
        elif os.path.exists(legacy_default):
            credential_path = legacy_default
        else:
            raise FileNotFoundError("Google credentials.json not found. Set OUTLET_CREDENTIALS_PATH or place it under apps/crawler/ or outlet-crawler/.")

    creds = ServiceAccountCredentials.from_json_keyfile_name(credential_path, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open(sheet_title)
    try:
        worksheet = spreadsheet.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="1000", cols="20")

    headers = ["ì œëª©", "ê¸°ê°„", "ìƒì„¸ ì œëª©", "ìƒì„¸ ê¸°ê°„", "ì¸ë„¤ì¼", "ìƒì„¸ ë§í¬", "í˜œíƒ ì„¤ëª…",
               "ë¸Œëœë“œ", "ì œí’ˆëª…", "ê°€ê²©", "ì´ë¯¸ì§€", "ì—…ë°ì´íŠ¸ ë‚ ì§œ", "event_id"]
    try:
        existing_data = worksheet.get_all_values()
        if existing_data and existing_data[0] == headers:
            existing_data = existing_data[1:]
    except:
        existing_data = []

    existing_links = {row[5] for row in existing_data if len(row) >= 6}
    filtered_new_rows = [row for row in new_rows if len(row) >= 6 and row[5] not in existing_links]
    print(f"âœ¨ [{sheet_name}] ìƒˆë¡œ ì¶”ê°€í•  í•­ëª© ìˆ˜: {len(filtered_new_rows)}ê°œ")

    if not filtered_new_rows:
        print(f"âœ… [{sheet_name}] ì¶”ê°€í•  ë°ì´í„° ì—†ìŒ.")
        return

    all_data = [headers] + filtered_new_rows + existing_data
    worksheet.clear()
    worksheet.update('A1', all_data)
    print(f"âœ… [{sheet_name}] ì´ {len(all_data)-1}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
    print(f"ğŸ”— ì‹œíŠ¸ ë§í¬: https://docs.google.com/spreadsheets/d/{spreadsheet.id}/edit")

# --- ì•„ìš¸ë › í¬ë¡¤ë§
def crawl_outlet(branchCd, outletName, sheet_name):
    import os as _os
    listing_mode = _os.environ.get("OUTLET_LISTING_MODE", "http").lower()
    detail_mode = _os.environ.get("OUTLET_DETAIL_MODE", "selenium").lower()
    driver = None
    # ë“œë¼ì´ë²„ëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë„ì›€ (listing click ë˜ëŠ” detail selenium)
    if listing_mode != "http" or detail_mode == "selenium":
        driver = setup_driver()
    new_rows = []
    # Max pages can be overridden via env (OUTLET_MAX_PAGES)
    import os as _os
    try:
        max_pages = int(_os.environ.get("OUTLET_MAX_PAGES", "4"))
    except Exception:
        max_pages = 4
    for page in range(1, max_pages + 1):
        print(f"[{sheet_name}] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        events = fetch_event_list(driver, branchCd, page)
        for event in events:
            # HTTP ëª¨ë“œ(dict)ì™€ í´ë¦­/í´ë°± ëª¨ë“œ(li)ë¥¼ ëª¨ë‘ ì§€ì›
            if isinstance(event, dict):
                title = event.get('title', '')
                period = event.get('period', '')
                image_url = event.get('image', '')
                detail_url = event.get('link', '')
            else:
                title_tag = event.select_one(".info_tit")
                period_tag = event.select_one(".info_txt")
                img_tag = event.select_one("img")
                link_tag = event.select_one("a")
                title = title_tag.get_text(separator=" ", strip=True) if title_tag else ""
                period = period_tag.get_text(strip=True) if period_tag else ""
                image_url = img_tag["src"] if img_tag else ""
                detail_url = "https://www.ehyundai.com" + link_tag["href"] if link_tag else ""
            if detail_mode == "http":
                detail = fetch_event_detail_http(detail_url)
            else:
                detail = fetch_event_detail(driver, detail_url)
            thumbnail_id = image_url.split("/")[-1].split(".")[0][-12:]
            event_id = thumbnail_id
            detail_data = {
                "id":       event_id,
                "ì œëª©":     title,
                "ê¸°ê°„":     period,
                "ìƒì„¸ ì œëª©": detail["ìƒì„¸ ì œëª©"],
                "ìƒì„¸ ê¸°ê°„": detail["ìƒì„¸ ê¸°ê°„"],
                "ì‹œì‘ì¼":   detail["ì‹œì‘ì¼"],    # â† ISO ì‹œì‘ì¼
                "ì¢…ë£Œì¼":   detail["ì¢…ë£Œì¼"],    # â† ISO ì¢…ë£Œì¼
                "ì§€ì ëª…":   outletName,          # â† ì§€ì ëª… ì¶”ê°€
                "ì¸ë„¤ì¼":   image_url,
                "ìƒì„¸ ë§í¬": detail_url,
                "í˜œíƒ ì„¤ëª…":" / ".join(detail["í…ìŠ¤íŠ¸ ì„¤ëª…"]),
                "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]
            }
            url_path = generate_html(detail_data, event_id)
            base_info = [title, period, detail["ìƒì„¸ ì œëª©"], detail["ìƒì„¸ ê¸°ê°„"],
                         image_url, detail_url, detail_data["í˜œíƒ ì„¤ëª…"]]
            if detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                for p in detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                    row = base_info + [
                        p["ë¸Œëœë“œ"], p["ì œí’ˆëª…"], p["ê°€ê²©"], p["ì´ë¯¸ì§€"],
                        datetime.today().strftime('%Y-%m-%d'), event_id]
                    new_rows.append(row)
            else:
                new_rows.append(base_info + ["", "", "", "",
                                 datetime.today().strftime('%Y-%m-%d'), event_id])
    if driver:
        driver.quit()
    # Allow skipping Google Sheets upload for dry-run via OUTLET_SKIP_SHEETS
    import os as _os
    if _os.environ.get("OUTLET_SKIP_SHEETS", "").lower() in ("1", "true", "yes"):
        print("âš ï¸ OUTLET_SKIP_SHEETS=1: Google Sheets ì—…ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
    else:
        upload_to_google_sheet("outlet-data", sheet_name, new_rows)

# --- ë©”ì¸ ì‹¤í–‰
def main():
    # ê¸°ì¡´ URL ë§¤í•‘ íŒŒì¼ ì½ê¸° (ëˆ„ì  ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
    import json
    global CFG
    if CFG is None:
        CFG = _get_config()

    mapping_path = CFG.get("MAPPING_PATH")
    global url_mapping
    try:
        with open(mapping_path, "r", encoding="utf-8") as f:
            url_mapping = json.load(f)
        print(f"ğŸ“‹ ê¸°ì¡´ URL ë§¤í•‘ ë¡œë“œ: {len(url_mapping)}ê°œ í•­ëª©")
    except:
        url_mapping = {}
        print("ğŸ“‹ ìƒˆë¡œìš´ URL ë§¤í•‘ íŒŒì¼ ìƒì„±")

    OUTLET_TARGETS = [
        ("B00174000", "ì†¡ë„",     "Sheet1"),
        ("B00172000", "ê¹€í¬",     "Sheet2"),
        ("B00178000", "ìŠ¤í˜ì´ìŠ¤ì›","Sheet3"),
    ]

    # í¬ë¡¤ë§ ì‹œì‘ ì „ ë§¤í•‘ ê°œìˆ˜
    initial_count = len(url_mapping)

    for branchCd, outletName, sheet_name in OUTLET_TARGETS:
        crawl_outlet(branchCd, outletName, sheet_name)
    
    # URL ë§¤í•‘ JSON íŒŒì¼ ì €ì¥ (ê¸°ì¡´ + ìƒˆë¡œìš´ ë§¤í•‘)
    with open(mapping_path, "w", encoding="utf-8") as f:
        json.dump(url_mapping, f, ensure_ascii=False, indent=2)
    
    new_count = len(url_mapping) - initial_count
    print(f"ğŸ“‹ URL ë§¤í•‘ íŒŒì¼ ì—…ë°ì´íŠ¸: ê¸°ì¡´ {initial_count}ê°œ + ì‹ ê·œ {new_count}ê°œ = ì´ {len(url_mapping)}ê°œ í•­ëª©")

    # âœ… ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap.xml ìƒì„±
    generate_sitemap(
        pages_dir=CFG.get("PAGES_DIR"),
        base_url=CFG.get("SITE_BASE_URL"),
        output_path=CFG.get("SITEMAP_PATH"),
    )

    # âœ… index.html (ì •ì  ì´ë²¤íŠ¸ ë§í¬ ëª©ë¡) ìƒì„±
    generate_index(
        pages_dir=CFG.get("PAGES_DIR"),
        index_path=CFG.get("INDEX_OUTPUT_PATH"),
    )

    print("\nğŸ‰ ì „ì²´ ì•„ìš¸ë › í¬ë¡¤ë§ ë° ì €ì¥ + ìƒˆë¡œìš´ URL êµ¬ì¡°ì˜ sitemap ìƒì„± ì™„ë£Œ!")
    print("ğŸ”— ìƒˆë¡œìš´ URL êµ¬ì¡°: discounts.deluxo.co.kr/pages/{ì§€ì ëª…}-{ì œëª©}.html")

if __name__ == "__main__":
    main()
